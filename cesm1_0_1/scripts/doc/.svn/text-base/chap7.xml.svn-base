<chapter id="ccsm_tests">
<title>CCSM Testing</title>

<!-- ======================================================================= -->
<sect1 id="ccsm_tests_summary">
<title>Testing overview</title>

<para>
CCSM4 has a few tools that support automated testing of the model.
In general, these should be used only after the model has
been ported to the target machine (see <xref linkend="port"/>).  
The tools are create_production_test,
create_test, and create_test_suite.  The create_production_test tool
is executed from a working case, and it tests exact restartability
of that case setup in a separate directory.  The create_test tool
allows a user to quickly setup and run one of several supported tests.
The create_test_suite tool quickly allows a user to setup and run
a list of supported tests.  Each of these tools will be described
below.
</para>

</sect1>

<sect1 id="create_production_test">
<title>create_production_test</title>

<para> In general, after configuring and testing a case and before 
starting a long production job based on that case, it's important
to verify that the model restarts exactly.  This is a standard
requirement of the system and will help demonstrate stability
of the configuration technically.  The tool
create_production_test is located in the case directory,
and it sets up an ERU two month exact restart test in a separate directory
based on the current case.  To use it, do the following
</para>

<screen>
> cd $CASEROOT
> ./create_production_test
> cd ../$CASE_ERU.$MACH
> $CASE_ERU.$MACH.build
submit $CASE_ERU.$MACH.run
Check your test results. A successful test produces "PASS" as
  the first word in the file, $CASE_ERU.$MACH/TestStatus
</screen>

<para>If the test fails, see <xref linkend="failed_tests"/> for
test debugging guidance.
</para>

</sect1>

<sect1 id="create_test">
<title>create_test</title>

<para>
The create_test tool is located in the scripts directory and
can be used to setup a standalone test case.  The test cases are
fixed and defined within the CCSM scripts.  To see the list of test cases
or for additional help, type "create_test -help" from the scripts
directory.  To use create_test, do something like
</para>

<screen>
> cd $CCSMROOT/scripts
> ./create_test -testname ERS.f19_g16.X.bluefire -testid t01
> cd ERS.f19_g16.X.bluefire.t01
> ERS.f19_g16.X.bluefire.t01.build
submit ERS.f19_g16.X.bluefire.t01.test
Check your test results. A successful test produces "PASS" as
  the first word in the file TestStatus
</screen>

<para>
The above sets up an exact restart test (ERS) at the 1.9x2.5_gx1v6
resolution using a dead model compset (X) for the machine bluefire.
The testid provides a unique tag for the test in case it needs to
be rerun (i.e. using -testid t02).  Some things to note about CCSM tests
</para>


<itemizedlist>
<listitem>
<para>For more information about the create_test tool, run "create_test -help".</para>
</listitem>
<listitem>
<para>Test results are set in the TestStatus file.  The TestStatus.out file 
provides additional details.</para>
</listitem>
<listitem>
<para>Tests are not always easily re-runable from an existing test directory.  Rather
than rerun a previous test case, it's best to setup a clean test case
(i.e. with a new testid).</para>
</listitem>
<listitem>
<para>The costs of tests vary widely.  Some are short and some are long.</para>
</listitem>
<listitem>
<para>If a test fails, see <xref linkend="failed_tests"/>.</para>
</listitem>
<listitem>
<para>There are -compare and -generate options for the create_test tool that support
regression testing.</para>
</listitem>
<listitem>
<para>There are extra test options that can be added to the test such as _D, _E, or
_P*.  These are described in more detail in the create_test -help output.</para>
</listitem>
</itemizedlist>

<para>The test status results have the following meaning
</para>

<informaltable>
<tgroup cols="2">
<thead>
<row><entry>Test Result</entry><entry>Description</entry></row>
</thead>
<tbody>
<row><entry> BFAIL </entry><entry>  compare test couldn't find base result</entry></row>
<row><entry> CHECK </entry><entry>  manual review of data is required</entry></row>
<row><entry> ERROR </entry><entry>  test checker failed, test may or may not have passed</entry></row>
<row><entry> FAIL  </entry><entry>  test failed</entry></row>
<row><entry> GEN   </entry><entry>  test has been generated</entry></row>
<row><entry> PASS  </entry><entry>  test passed</entry></row>
<row><entry> PEND  </entry><entry>  test has been submitted</entry></row>
<row><entry> RUN   </entry><entry>  test is currently running OR it hung, timed out, or died ungracefully </entry></row>
<row><entry> SFAIL </entry><entry>  generation of test failed in scripts</entry></row>
<row><entry> TFAIL </entry><entry>  test setup error</entry></row>
<row><entry> UNDEF </entry><entry>  undefined result</entry></row>
</tbody>
</tgroup>
</informaltable>

<para>
The following tests are available at the time of writing
</para>

<informaltable>
<tgroup cols="2">
<thead>
<row><entry>Test</entry><entry>Description</entry></row>
</thead>
<tbody>
<row><entry>SMS </entry><entry> 5 day smoke test</entry></row>
<row><entry>ERS </entry><entry> 10 day exact restart from startup </entry></row>
<row><entry>ERP </entry><entry> 2 month exact restart from startup </entry></row>
<row><entry>ERB </entry><entry> branch/exact restart test </entry></row>
<row><entry>ERH </entry><entry> hybrid/exact restart test </entry></row>
<row><entry>ERI </entry><entry> hybrid/branch/exact restart test</entry></row>
<row><entry>ERT </entry><entry> 2 month exact restart from startup, history file test </entry></row>
<row><entry>ERU </entry><entry> 2 month exact restart from initial conditions, history file test </entry></row>
<row><entry>SEQ </entry><entry> sequencing bit-for-bit test </entry></row>
<row><entry>PEA </entry><entry> single processor testing</entry></row>
<row><entry>PEM </entry><entry> pe counts mpi bit-for-bit test </entry></row>
<row><entry>PET </entry><entry> pe counts mpi/openmp bit-for-bit test</entry></row>
<row><entry>CME </entry><entry> compare mct and esmf interfaces test</entry></row>
</tbody>
</tgroup>
</informaltable>

</sect1>

<sect1 id="create_test_suite">
<title>create_test_suite</title>

<para>
The create_test_suite tool is located in the scripts directory and
can be used to setup a suite of standalone test cases automatically.
To use this tool, a list of tests needs to exist in a file.  Some
examples can be found in the directory scripts/ccsm_utils/Testlists.
create_test_suite in invoked on a list of tests and then the full
list of tests is generated.  In addition, an automated submission
script and reporting script are created.  The cs.submit script reduces
the time to submit multiple test cases significantly.  To use this tool, 
do something like the following
</para>

<screen>
create a list of desired tests in some filename, i.e. my_lists
> create_test_suite -input_list my_lists -testid suite01
> ./cs.status.suite01
> ./cs.submit.suite01.$MACH
> ./cs.status.suite01
</screen>

<para>
The cs.status script is generated by create_test_suite, and it reports
the status of all the tests in the suite.  The cs.submit script builds
and submits all the tests sequentially.  The cs.submit script should only
be executed once to build and submit all the tests.  The cs.status script
can be executed as often as needed to check the status of the tests.
When all the tests have completed running, then the results are static
and complete.  To help debug failed tests, see <xref linkend="failed_tests"/>..
</para>

</sect1>

<sect1 id="failed_tests">
<title>Debugging Tests That Fail</title>

<para>
This section describes what steps can be taken to try to identify
why a test failed.  The primary information associated with reviewing
and debugging a run can be found in <xref linkend="troubleshooting_run_time"/>.
</para>

<para>
First, verify that a test case is no longer in the batch queue.  If that's
the case, then review the <link linkend="create_test">possible test results</link>
and compare that to the result in the TestStatus file.  Next, review the 
TestStatus.out file to see if there is any additional information about what
the test did.  Finally, go to the <link linkend="troubleshooting_run_time">
troubleshooting</link> section and work through the various log files.
</para>

<para>
Finally, there are a couple other things to mention.  If the TestStatus file contains 
"RUN" but the job is no longer in the queue, it's possible that the job
either timed out because the wall clock on the batch submission was too
short, or the job hung due to some run-time error.  Check the batch log files
to see if the job was killed due to a time limit, and if it was increase
the time limit and either resubmit the job or generate a new test case
and update the time limit before submitting it.  </para>

<para>Also, a test case can fail
because either the job didn't run properly or because the test conditions
(i.e. exact restart) weren't met.  Try to determine whether the test failed
because the run failed or because the test did not meet the test conditions.
If a test is failing early in a run, it's usually best to setup a
standalone case with the same configuration in order to debug problems.
If the test is running fine, but the test conditions are not being met
(i.e. exact restart), then that requires debugging of the model in the
context of the test conditions.
</para>

<para>
Not all tests will pass for all model configurations.  Some of the issues
we are aware of are
</para>

<itemizedlist>
<listitem>
<para>All models are bit-for-bit reproducible with different processor counts
EXCEPT pop.  The BFBFLAG must be set to TRUE in the env_run.xml file if the
coupler is to meet this condition.  There will be a performance penalty when
this flag is set.</para>
</listitem>
<listitem>
<para>Some of the active components cannot run with the mpi serial library.
This library takes the place of mpi calls when the model is running on 
one processors and MPI is not available or not desirable.  The mpi serial
library is part of the CCSM release and is invoked by setting the 
USE_MPISERIAL variable in env_build.xml to TRUE.  An effort is underway
to extend the mpi serial library to support all components' usage of the
mpi library with this standalone implementation.</para>
</listitem>
</itemizedlist>

</sect1>

</chapter>


